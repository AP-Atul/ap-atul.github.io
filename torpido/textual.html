<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>torpido.textual API documentation</title>
<meta name="description" content="This file is for calculating the textual ranking for the video
,the textual ranking is done by detecting text in the video.
Text detection is achieved â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>torpido.textual</code></h1>
</header>
<section id="section-intro">
<p>This file is for calculating the textual ranking for the video
,the textual ranking is done by detecting text in the video.
Text detection is achieved by using EAST Text Detection model
of OpenCV, this model can detect text and return confidences and
geometry for the sections that contain the text.
If mixed with text extraction it can give text from the image.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This file is for calculating the textual ranking for the video
,the textual ranking is done by detecting text in the video.
Text detection is achieved by using EAST Text Detection model
of OpenCV, this model can detect text and return confidences and
geometry for the sections that contain the text.
If mixed with text extraction it can give text from the image.
&#34;&#34;&#34;

import time

import cv2
import numpy as np

from torpido.config import *
from torpido.exceptions import EastModelEnvironmentMissing
from torpido.util import image
from torpido.video import VideoGet


class Textual:
    &#34;&#34;&#34;
    Class to perform Textual analysis on the input video file. This class creates its own
    video reader and handles the frame independent of the `Visual`. The EAST model of the
    OpenCV is used to detect text in the video.

    Since, the model is very slow depend-ing on the system its running. So some of the frames
    are skipped `TEXT_SKIP_FRAMES` determines the no of frames to skip

    Attributes
    ----------
    __fps : float
        video fps
    __frameCount : int
        number of frames in the video
    __textRanks : list
        list of the ranks
    __videoGetter : VideoGet
        object of the video get to read the video through thread
    __minConfidence : int
        minimum confidence to determine if the video contains text
    __WIDTH : int, default=320
        the east model requires the frame to be size of multiple of 32x32
    __HEIGHT : int, default=320
        height of the frame
    __skipFrames : int
        no of frames to skip
    __textRankPath : str
        constants file defines where to store the ranks
    __net : object
        loaded east model
    __textDetectLayerName
        layer name to detect the text in the video and return the code
    __textDisplayLayerNames
        layers to detect and return the coordinates of the boxes of text detected
    &#34;&#34;&#34;

    def __init__(self):
        cv2.setUseOptimized(True)
        self.__fps = None
        self.__frameCount = None
        self.__textRanks = None
        self.__videoGetter = None
        self.__minConfidence = TEXT_MIN_CONFIDENCE
        self.__WIDTH = 320  # this val should be multiple of 32
        self.__HEIGHT = 320  # same thing for this
        self.__skipFrames = TEXT_SKIP_FRAMES
        self.__textRankPath = os.path.join(os.getcwd(), RANK_DIR, RANK_OUT_TEXT)

        # initializing the model
        # reading the model in the memory
        if TEXT_EAST_MODEL_PATH is not None:
            self.__net = cv2.dnn.readNet(TEXT_EAST_MODEL_PATH)
        else:
            raise EastModelEnvironmentMissing

        # adding output layer to only return confidence for text
        self.__textDetectLayerName = [&#34;feature_fusion/Conv_7/Sigmoid&#34;]

        # adding output layers to the model with text detected boxes
        self.__textDisplayLayerNames = [&#34;feature_fusion/Conv_7/Sigmoid&#34;,
                                        &#34;feature_fusion/concat_3&#34;]

    def startProcessing(self, inputFile, display=False):
        &#34;&#34;&#34;
        Function to perform the Textual Processing on the input video file.
        The video can be displayed as the processing is going on.

        Parameters
        ----------
        inputFile : str
            input video file
        display : bool
            True to display the video while processing
        &#34;&#34;&#34;

        if os.path.isfile(inputFile) is False:
            Log.e(f&#34;File {inputFile} does not exists&#34;)
            return

        self.__videoGetter = VideoGet(str(inputFile)).start()
        myClip = self.__videoGetter.stream

        if self.__videoGetter.getQueueSize() == 0:
            time.sleep(0.5)
            Log.d(&#34;Waiting for the buffer to fill up.&#34;)

        self.__fps = myClip.get(cv2.CAP_PROP_FPS)
        self.__frameCount = myClip.get(cv2.CAP_PROP_FRAME_COUNT)
        self.__skipFrames = int(self.__fps * self.__skipFrames)

        # maintaining the ranks for text detection
        count = 0
        self.__textRanks = []

        while self.__videoGetter.more():
            frame = self.__videoGetter.read()

            if frame is None:
                break

            # resizing the frame to a multiple of 32 x 32
            # resizing the frame
            original = frame
            (H, W) = frame.shape[:2]
            rW = W / float(self.__WIDTH)
            rH = H / float(self.__HEIGHT)
            frame = cv2.resize(frame, (W, H))
            count += 1

            if count % self.__skipFrames == 0:

                #  making the image blob
                blob = cv2.dnn.blobFromImage(frame,
                                             1.0,
                                             (self.__WIDTH, self.__HEIGHT),
                                             (123.68, 116.78, 103.94),
                                             swapRB=True, crop=False)

                # run text detection
                if display:
                    detectedText = self.__runTextDetectDisplay(blob, (rW, rH), original)
                else:
                    detectedText = self.__runTextDetect(blob)

                # if text is detected
                if detectedText:
                    self.__textRanks.extend([RANK_TEXT] * int(self.__skipFrames))
                    Log.d(&#34;Text detected.&#34;)
                else:
                    self.__textRanks.extend([0] * int(self.__skipFrames))
                    Log.d(&#34;No text detected.&#34;)

        # clearing the memory
        myClip.release()
        self.__videoGetter.stop()
        cv2.destroyAllWindows()

        # calling the normalization of ranking
        self.__timedRankingNormalize()

    def __runTextDetect(self, blob):
        &#34;&#34;&#34;
        Function to detect only text and no display. Gets the scores and calculates if the image
        contains any text

        Parameters
        ----------
        blob : blob
            blob of the image

        Returns
        -------
        bool
            True denotes text detected
        &#34;&#34;&#34;
        self.__net.setInput(blob)
        scores = self.__net.forward(self.__textDetectLayerName)
        numRows, numCols = np.asarray(scores).shape[3: 5]
        confidences = []

        # since image is 320x320 the output is 80x80 (scores)
        for x in range(0, numRows):
            scoreData = scores[0][0][0][x]
            for y in range(0, numCols):
                if scoreData[y] &lt; self.__minConfidence:
                    continue

                confidences.append(scoreData[y])

        # if confidences contain some value
        if len(confidences) &gt; 0:
            return True
        return False

    def __runTextDetectDisplay(self, blob, rSize, original):
        &#34;&#34;&#34;
        Function to detect text using layer for getting the rectangles
        to display on the frame

        Parameters
        ----------
        blob : blob
            blob of the image
        rSize : tuple
            real sizes of the images
        original : image array
            un-resized image to display

        Returns
        -------
        bool
            True denotes text detected
        &#34;&#34;&#34;
        # running the model
        self.__net.setInput(blob=blob)
        scores, geometry = self.__net.forward(self.__textDisplayLayerNames)

        numRows, numCols = scores.shape[2:4]
        rect = []
        confidences = []

        # since image is 320x320 the output is 80x80 (scores)
        for y in range(0, numRows):
            scoresData = scores[0, 0, y]
            xData0 = geometry[0, 0, y]
            xData1 = geometry[0, 1, y]
            xData2 = geometry[0, 2, y]
            xData3 = geometry[0, 3, y]
            anglesData = geometry[0, 4, y]

            for x in range(0, numCols):
                # if our score does not have sufficient probability, ignore it
                if scoresData[x] &lt; self.__minConfidence:
                    continue

                # compute the offset factor as our resulting feature maps will
                # be 4x smaller than the input image
                (offsetX, offsetY) = (x * 4, y * 4)

                # extract the rotation angle for the prediction and then
                # compute the sin and cosine
                angle = anglesData[x]
                cos = np.cos(angle)
                sin = np.sin(angle)

                # use the geometry volume to derive the width and height of
                # the bounding box
                h = xData0[x] + xData2[x]
                w = xData1[x] + xData3[x]

                # compute both the starting and ending (x, y)-coordinates for
                # the text prediction bounding box
                endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
                endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
                startX = int(endX - w)
                startY = int(endY - h)

                # add the bounding box coordinates and probability score to
                # our respective lists
                rect.append((startX, startY, endX, endY))
                confidences.append(scoresData[x])

        # compressing the boxes or rectangles
        boxes = image.nonMaxSuppression(np.array(rect), probs=confidences)

        rW, rH = rSize
        for startX, startY, endX, endY in boxes:
            startX = int(startX * rW)
            startY = int(startY * rH)
            endX = int(endX * rW)
            endY = int(endY * rH)

            # draw the bounding box on the image
            cv2.rectangle(original, (startX, startY), (endX, endY), (0, 255, 0), 2)

        cv2.imshow(&#34;Text Detection&#34;, original)
        cv2.waitKey(1) &amp; 0xFF

        if len(confidences) &gt; 0:
            return True
        return False

    def __timedRankingNormalize(self):
        &#34;&#34;&#34;
        Since ranking is added to frames, since frames are duration * fps
        and audio frame system is different since frame are duration * rate
        so we need to generalize the ranking system

        sol: ranking sec of the video and audio, for than taking mean of the
        frames to generate rank for video.
        since ranking is 0 or 1, the mean will be different and we get more versatile
        results.

        we will read the list and slice the video to get 1 sec of frames and get
        mean/average as the rank for the 1 sec
        &#34;&#34;&#34;
        textNormalize = []
        for i in range(0, int(self.__frameCount), int(self.__fps)):
            if len(self.__textRanks) &gt;= (i + int(self.__fps)):
                textNormalize.append(np.mean(self.__textRanks[i: i + int(self.__fps)]))
            else:
                break

        # saving all processed stuffs
        dump(textNormalize, self.__textRankPath)
        Log.d(f&#34;Textual rank length {len(textNormalize)}&#34;)
        Log.i(&#34;Textual ranking saved .............&#34;)

    def __del__(self):
        &#34;&#34;&#34;
        clean ups
        &#34;&#34;&#34;
        del self.__net
        del self.__videoGetter
        Log.d(&#34;Cleaning up.&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torpido.textual.Textual"><code class="flex name class">
<span>class <span class="ident">Textual</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class to perform Textual analysis on the input video file. This class creates its own
video reader and handles the frame independent of the <code>Visual</code>. The EAST model of the
OpenCV is used to detect text in the video.</p>
<p>Since, the model is very slow depend-ing on the system its running. So some of the frames
are skipped <code>TEXT_SKIP_FRAMES</code> determines the no of frames to skip</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>__fps</code></strong> :&ensp;<code>float</code></dt>
<dd>video fps</dd>
<dt><strong><code>__frameCount</code></strong> :&ensp;<code>int</code></dt>
<dd>number of frames in the video</dd>
<dt><strong><code>__textRanks</code></strong> :&ensp;<code>list</code></dt>
<dd>list of the ranks</dd>
<dt><strong><code>__videoGetter</code></strong> :&ensp;<code>VideoGet</code></dt>
<dd>object of the video get to read the video through thread</dd>
<dt><strong><code>__minConfidence</code></strong> :&ensp;<code>int</code></dt>
<dd>minimum confidence to determine if the video contains text</dd>
<dt><strong><code>__WIDTH</code></strong> :&ensp;<code>int</code>, default=<code>320</code></dt>
<dd>the east model requires the frame to be size of multiple of 32x32</dd>
<dt><strong><code>__HEIGHT</code></strong> :&ensp;<code>int</code>, default=<code>320</code></dt>
<dd>height of the frame</dd>
<dt><strong><code>__skipFrames</code></strong> :&ensp;<code>int</code></dt>
<dd>no of frames to skip</dd>
<dt><strong><code>__textRankPath</code></strong> :&ensp;<code>str</code></dt>
<dd>constants file defines where to store the ranks</dd>
<dt><strong><code>__net</code></strong> :&ensp;<code>object</code></dt>
<dd>loaded east model</dd>
<dt><strong><code>__textDetectLayerName</code></strong></dt>
<dd>layer name to detect the text in the video and return the code</dd>
<dt><strong><code>__textDisplayLayerNames</code></strong></dt>
<dd>layers to detect and return the coordinates of the boxes of text detected</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Textual:
    &#34;&#34;&#34;
    Class to perform Textual analysis on the input video file. This class creates its own
    video reader and handles the frame independent of the `Visual`. The EAST model of the
    OpenCV is used to detect text in the video.

    Since, the model is very slow depend-ing on the system its running. So some of the frames
    are skipped `TEXT_SKIP_FRAMES` determines the no of frames to skip

    Attributes
    ----------
    __fps : float
        video fps
    __frameCount : int
        number of frames in the video
    __textRanks : list
        list of the ranks
    __videoGetter : VideoGet
        object of the video get to read the video through thread
    __minConfidence : int
        minimum confidence to determine if the video contains text
    __WIDTH : int, default=320
        the east model requires the frame to be size of multiple of 32x32
    __HEIGHT : int, default=320
        height of the frame
    __skipFrames : int
        no of frames to skip
    __textRankPath : str
        constants file defines where to store the ranks
    __net : object
        loaded east model
    __textDetectLayerName
        layer name to detect the text in the video and return the code
    __textDisplayLayerNames
        layers to detect and return the coordinates of the boxes of text detected
    &#34;&#34;&#34;

    def __init__(self):
        cv2.setUseOptimized(True)
        self.__fps = None
        self.__frameCount = None
        self.__textRanks = None
        self.__videoGetter = None
        self.__minConfidence = TEXT_MIN_CONFIDENCE
        self.__WIDTH = 320  # this val should be multiple of 32
        self.__HEIGHT = 320  # same thing for this
        self.__skipFrames = TEXT_SKIP_FRAMES
        self.__textRankPath = os.path.join(os.getcwd(), RANK_DIR, RANK_OUT_TEXT)

        # initializing the model
        # reading the model in the memory
        if TEXT_EAST_MODEL_PATH is not None:
            self.__net = cv2.dnn.readNet(TEXT_EAST_MODEL_PATH)
        else:
            raise EastModelEnvironmentMissing

        # adding output layer to only return confidence for text
        self.__textDetectLayerName = [&#34;feature_fusion/Conv_7/Sigmoid&#34;]

        # adding output layers to the model with text detected boxes
        self.__textDisplayLayerNames = [&#34;feature_fusion/Conv_7/Sigmoid&#34;,
                                        &#34;feature_fusion/concat_3&#34;]

    def startProcessing(self, inputFile, display=False):
        &#34;&#34;&#34;
        Function to perform the Textual Processing on the input video file.
        The video can be displayed as the processing is going on.

        Parameters
        ----------
        inputFile : str
            input video file
        display : bool
            True to display the video while processing
        &#34;&#34;&#34;

        if os.path.isfile(inputFile) is False:
            Log.e(f&#34;File {inputFile} does not exists&#34;)
            return

        self.__videoGetter = VideoGet(str(inputFile)).start()
        myClip = self.__videoGetter.stream

        if self.__videoGetter.getQueueSize() == 0:
            time.sleep(0.5)
            Log.d(&#34;Waiting for the buffer to fill up.&#34;)

        self.__fps = myClip.get(cv2.CAP_PROP_FPS)
        self.__frameCount = myClip.get(cv2.CAP_PROP_FRAME_COUNT)
        self.__skipFrames = int(self.__fps * self.__skipFrames)

        # maintaining the ranks for text detection
        count = 0
        self.__textRanks = []

        while self.__videoGetter.more():
            frame = self.__videoGetter.read()

            if frame is None:
                break

            # resizing the frame to a multiple of 32 x 32
            # resizing the frame
            original = frame
            (H, W) = frame.shape[:2]
            rW = W / float(self.__WIDTH)
            rH = H / float(self.__HEIGHT)
            frame = cv2.resize(frame, (W, H))
            count += 1

            if count % self.__skipFrames == 0:

                #  making the image blob
                blob = cv2.dnn.blobFromImage(frame,
                                             1.0,
                                             (self.__WIDTH, self.__HEIGHT),
                                             (123.68, 116.78, 103.94),
                                             swapRB=True, crop=False)

                # run text detection
                if display:
                    detectedText = self.__runTextDetectDisplay(blob, (rW, rH), original)
                else:
                    detectedText = self.__runTextDetect(blob)

                # if text is detected
                if detectedText:
                    self.__textRanks.extend([RANK_TEXT] * int(self.__skipFrames))
                    Log.d(&#34;Text detected.&#34;)
                else:
                    self.__textRanks.extend([0] * int(self.__skipFrames))
                    Log.d(&#34;No text detected.&#34;)

        # clearing the memory
        myClip.release()
        self.__videoGetter.stop()
        cv2.destroyAllWindows()

        # calling the normalization of ranking
        self.__timedRankingNormalize()

    def __runTextDetect(self, blob):
        &#34;&#34;&#34;
        Function to detect only text and no display. Gets the scores and calculates if the image
        contains any text

        Parameters
        ----------
        blob : blob
            blob of the image

        Returns
        -------
        bool
            True denotes text detected
        &#34;&#34;&#34;
        self.__net.setInput(blob)
        scores = self.__net.forward(self.__textDetectLayerName)
        numRows, numCols = np.asarray(scores).shape[3: 5]
        confidences = []

        # since image is 320x320 the output is 80x80 (scores)
        for x in range(0, numRows):
            scoreData = scores[0][0][0][x]
            for y in range(0, numCols):
                if scoreData[y] &lt; self.__minConfidence:
                    continue

                confidences.append(scoreData[y])

        # if confidences contain some value
        if len(confidences) &gt; 0:
            return True
        return False

    def __runTextDetectDisplay(self, blob, rSize, original):
        &#34;&#34;&#34;
        Function to detect text using layer for getting the rectangles
        to display on the frame

        Parameters
        ----------
        blob : blob
            blob of the image
        rSize : tuple
            real sizes of the images
        original : image array
            un-resized image to display

        Returns
        -------
        bool
            True denotes text detected
        &#34;&#34;&#34;
        # running the model
        self.__net.setInput(blob=blob)
        scores, geometry = self.__net.forward(self.__textDisplayLayerNames)

        numRows, numCols = scores.shape[2:4]
        rect = []
        confidences = []

        # since image is 320x320 the output is 80x80 (scores)
        for y in range(0, numRows):
            scoresData = scores[0, 0, y]
            xData0 = geometry[0, 0, y]
            xData1 = geometry[0, 1, y]
            xData2 = geometry[0, 2, y]
            xData3 = geometry[0, 3, y]
            anglesData = geometry[0, 4, y]

            for x in range(0, numCols):
                # if our score does not have sufficient probability, ignore it
                if scoresData[x] &lt; self.__minConfidence:
                    continue

                # compute the offset factor as our resulting feature maps will
                # be 4x smaller than the input image
                (offsetX, offsetY) = (x * 4, y * 4)

                # extract the rotation angle for the prediction and then
                # compute the sin and cosine
                angle = anglesData[x]
                cos = np.cos(angle)
                sin = np.sin(angle)

                # use the geometry volume to derive the width and height of
                # the bounding box
                h = xData0[x] + xData2[x]
                w = xData1[x] + xData3[x]

                # compute both the starting and ending (x, y)-coordinates for
                # the text prediction bounding box
                endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
                endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
                startX = int(endX - w)
                startY = int(endY - h)

                # add the bounding box coordinates and probability score to
                # our respective lists
                rect.append((startX, startY, endX, endY))
                confidences.append(scoresData[x])

        # compressing the boxes or rectangles
        boxes = image.nonMaxSuppression(np.array(rect), probs=confidences)

        rW, rH = rSize
        for startX, startY, endX, endY in boxes:
            startX = int(startX * rW)
            startY = int(startY * rH)
            endX = int(endX * rW)
            endY = int(endY * rH)

            # draw the bounding box on the image
            cv2.rectangle(original, (startX, startY), (endX, endY), (0, 255, 0), 2)

        cv2.imshow(&#34;Text Detection&#34;, original)
        cv2.waitKey(1) &amp; 0xFF

        if len(confidences) &gt; 0:
            return True
        return False

    def __timedRankingNormalize(self):
        &#34;&#34;&#34;
        Since ranking is added to frames, since frames are duration * fps
        and audio frame system is different since frame are duration * rate
        so we need to generalize the ranking system

        sol: ranking sec of the video and audio, for than taking mean of the
        frames to generate rank for video.
        since ranking is 0 or 1, the mean will be different and we get more versatile
        results.

        we will read the list and slice the video to get 1 sec of frames and get
        mean/average as the rank for the 1 sec
        &#34;&#34;&#34;
        textNormalize = []
        for i in range(0, int(self.__frameCount), int(self.__fps)):
            if len(self.__textRanks) &gt;= (i + int(self.__fps)):
                textNormalize.append(np.mean(self.__textRanks[i: i + int(self.__fps)]))
            else:
                break

        # saving all processed stuffs
        dump(textNormalize, self.__textRankPath)
        Log.d(f&#34;Textual rank length {len(textNormalize)}&#34;)
        Log.i(&#34;Textual ranking saved .............&#34;)

    def __del__(self):
        &#34;&#34;&#34;
        clean ups
        &#34;&#34;&#34;
        del self.__net
        del self.__videoGetter
        Log.d(&#34;Cleaning up.&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="torpido.textual.Textual.startProcessing"><code class="name flex">
<span>def <span class="ident">startProcessing</span></span>(<span>self, inputFile, display=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to perform the Textual Processing on the input video file.
The video can be displayed as the processing is going on.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>inputFile</code></strong> :&ensp;<code>str</code></dt>
<dd>input video file</dd>
<dt><strong><code>display</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to display the video while processing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def startProcessing(self, inputFile, display=False):
    &#34;&#34;&#34;
    Function to perform the Textual Processing on the input video file.
    The video can be displayed as the processing is going on.

    Parameters
    ----------
    inputFile : str
        input video file
    display : bool
        True to display the video while processing
    &#34;&#34;&#34;

    if os.path.isfile(inputFile) is False:
        Log.e(f&#34;File {inputFile} does not exists&#34;)
        return

    self.__videoGetter = VideoGet(str(inputFile)).start()
    myClip = self.__videoGetter.stream

    if self.__videoGetter.getQueueSize() == 0:
        time.sleep(0.5)
        Log.d(&#34;Waiting for the buffer to fill up.&#34;)

    self.__fps = myClip.get(cv2.CAP_PROP_FPS)
    self.__frameCount = myClip.get(cv2.CAP_PROP_FRAME_COUNT)
    self.__skipFrames = int(self.__fps * self.__skipFrames)

    # maintaining the ranks for text detection
    count = 0
    self.__textRanks = []

    while self.__videoGetter.more():
        frame = self.__videoGetter.read()

        if frame is None:
            break

        # resizing the frame to a multiple of 32 x 32
        # resizing the frame
        original = frame
        (H, W) = frame.shape[:2]
        rW = W / float(self.__WIDTH)
        rH = H / float(self.__HEIGHT)
        frame = cv2.resize(frame, (W, H))
        count += 1

        if count % self.__skipFrames == 0:

            #  making the image blob
            blob = cv2.dnn.blobFromImage(frame,
                                         1.0,
                                         (self.__WIDTH, self.__HEIGHT),
                                         (123.68, 116.78, 103.94),
                                         swapRB=True, crop=False)

            # run text detection
            if display:
                detectedText = self.__runTextDetectDisplay(blob, (rW, rH), original)
            else:
                detectedText = self.__runTextDetect(blob)

            # if text is detected
            if detectedText:
                self.__textRanks.extend([RANK_TEXT] * int(self.__skipFrames))
                Log.d(&#34;Text detected.&#34;)
            else:
                self.__textRanks.extend([0] * int(self.__skipFrames))
                Log.d(&#34;No text detected.&#34;)

    # clearing the memory
    myClip.release()
    self.__videoGetter.stop()
    cv2.destroyAllWindows()

    # calling the normalization of ranking
    self.__timedRankingNormalize()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torpido" href="index.html">torpido</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torpido.textual.Textual" href="#torpido.textual.Textual">Textual</a></code></h4>
<ul class="">
<li><code><a title="torpido.textual.Textual.startProcessing" href="#torpido.textual.Textual.startProcessing">startProcessing</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>