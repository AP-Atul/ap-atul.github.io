<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>lib.visual API documentation</title>
<meta name="description" content="This file reads the video and gives ranking to frames
that have motion in it, saves in the dictionary with frame numbers
this dictionary is then saved â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>lib.visual</code></h1>
</header>
<section id="section-intro">
<p>This file reads the video and gives ranking to frames
that have motion in it, saves in the dictionary with frame numbers
this dictionary is then saved in a joblib file defined in constants.py</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This file reads the video and gives ranking to frames
that have motion in it, saves in the dictionary with frame numbers
this dictionary is then saved in a joblib file defined in constants.py
&#34;&#34;&#34;

import os
import time

import cv2
import numpy as np
from joblib import dump

from lib.util.cache import Cache
from lib.util.constants import *
from lib.util.logger import Log
from lib.util.videoReader import VideoGet


class Visual:
    &#34;&#34;&#34;
    Class to perform Visual Processing on the input video file. Motion and Blur detections
    are used to calculate the rank. The ranks are per frame, so later the ranks are
    normalized to sec

    Attributes
    ----------
    motionRankPath : str
        file to store the motion rank
    blurRankPath : str
        file to store the blur rank
    blurThreshold : int
        threshold to rank the blur feature
    motionThreshold : int
        threshold to rank the motion feature
    fps : float
        input video fps
    frameCount : int
        number of frames
    motion : list
        list of the ranks for the motion feature
    blur : list
        list of the ranks for the blur feature
    cache : Cache
        cache object to store the data
    videoGetter : VideoGet
        video reader object to read the video and save it in thread
    &#34;&#34;&#34;

    def __init__(self):
        self.motionRankPath = os.path.join(os.getcwd(), RANK_DIR, RANK_OUT_MOTION)
        self.blurRankPath = os.path.join(os.getcwd(), RANK_DIR, RANK_OUT_BLUR)
        self.blurThreshold = BLUR_THRESHOLD
        self.motionThreshold = MOTION_THRESHOLD
        self.fps = None
        self.frameCount = None
        self.motion = None
        self.blur = None
        self.cache = Cache()
        self.videoGetter = None

    def detectBlur(self, image):
        &#34;&#34;&#34;
        Laplacian take 2nd derivative of one channel of the image(gray scale)
        It highlights regions of an image containing rapid intensity changes, much like the Sobel and Scharr operators.
        And then calculates the variance (squared SD), then check if the variance satisfies the Threshold value/

        Parameters
        ---------
        image : array
            frame from the video file
        &#34;&#34;&#34;
        # (h, w) = image.shape
        # (cX, cY) = (int(w / 2.0), int(h / 2.0))
        #
        # fft = np.fft.fft2(image)
        # fftShift = np.fft.fftshift(fft)
        #
        # fftShift[cY - size:cY + size, cX - size:cX + size] = 0
        # fftShift = np.fft.ifftshift(fftShift)
        # recon = np.fft.ifft2(fftShift)
        #
        # magnitude = 20 * np.log(np.abs(recon))
        # mean = np.mean(magnitude)
        # return mean &lt;= thresh
        if cv2.Laplacian(image, cv2.CV_64F).var() &gt;= self.blurThreshold:
            return RANK_BLUR
        return 0

    def startProcessing(self, inputFile, display=False):
        &#34;&#34;&#34;
        Function to run the processing on the Video file. Motion and Blur features are
        detected and based on that ranking is set

        Parameters
        ----------
        inputFile : str
            input video file
        display : bool
            True to display the video while processing
        &#34;&#34;&#34;

        if os.path.isfile(inputFile) is False:
            Log.e(f&#34;File {inputFile} does not exists&#34;)
            return

        # maintaining the motion and blur frames list
        self.motion = list()
        self.blur = list()

        self.videoGetter = VideoGet(str(inputFile)).start()
        myClip = self.videoGetter.stream

        if self.videoGetter.Q.qsize() == 0:
            time.sleep(1)
            Log.d(f&#34;Waiting for the buffer to fill up.&#34;)

        fps = myClip.get(cv2.CAP_PROP_FPS)
        totalFrames = myClip.get(cv2.CAP_PROP_FRAME_COUNT)

        self.fps = fps
        self.frameCount = totalFrames
        self.setVideoFps()
        self.setVideoFrameCount()

        # printing some info
        Log.d(f&#34;Total count of video frames :: {totalFrames}&#34;)
        Log.i(f&#34;Video fps :: {fps}&#34;)
        Log.i(f&#34;Bit rate :: {cv2.CAP_PROP_BITRATE}&#34;)
        Log.i(f&#34;Video format :: {cv2.CAP_PROP_FORMAT}&#34;)
        Log.i(f&#34;Video four cc :: {cv2.CAP_PROP_FOURCC}&#34;)

        count = 0
        firstFrame = self.videoGetter.read()
        firstFrameProcessed = True

        while self.videoGetter.more():
            frame = self.videoGetter.read()

            if frame is None:
                break

            original = frame
            count += 1

            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            self.blur.append(self.detectBlur(frame))
            frame = cv2.GaussianBlur(frame, (21, 21), 0)

            if firstFrameProcessed:
                firstFrame = cv2.cvtColor(firstFrame, cv2.COLOR_BGR2GRAY)
                firstFrame = cv2.GaussianBlur(firstFrame, (21, 21), 0)
                firstFrameProcessed = False

            frameDelta = cv2.absdiff(firstFrame, frame)
            thresh = cv2.threshold(frameDelta, self.motionThreshold, 255, cv2.THRESH_BINARY)[1]

            threshSum = thresh.sum()
            if threshSum &gt; 0:
                self.motion.append(RANK_MOTION)
            else:
                self.motion.append(0)

            if display:
                cv2.imshow(&#34;Video Feed&#34;, original)
                key = cv2.waitKey(1) &amp; 0xFF

                # if the `q` key is pressed, break from the loop
                if key == ord(&#34;q&#34;):
                    break

            # assigning the processed frame as the first frame to cal diff later on
            firstFrame = frame

        # clearing memory
        myClip.release()
        self.videoGetter.stop()
        cv2.destroyAllWindows()

        # calling the normalization of ranking
        self.timedRankingNormalize()

    def timedRankingNormalize(self):
        &#34;&#34;&#34;
        Since ranking is added to frames, since frames are duration * fps
        and audio frame system is different since frame are duration * rate
        so we need to generalize the ranking system
        sol: ranking sec of the video and audio, for than taking mean of the
        frames to generate rank for video.

        Since ranking is 0 or 1, the mean will be different and we get more versatile
        results.

        We will read both the list and slice the video to get 1 sec of frames(1 * fps) and get
        mean/average as the rank for the 1 sec

        &#34;&#34;&#34;
        motionNormalize = []
        blurNormalize = []
        for i in range(0, int(self.frameCount), int(self.fps)):
            if len(self.motion) &gt;= (i + int(self.fps)):
                motionNormalize.append(np.mean(self.motion[i: i + int(self.fps)]))
                blurNormalize.append(np.mean(self.blur[i: i + int(self.fps)]))
            else:
                break

        # saving all processed stuffs
        dump(motionNormalize, self.motionRankPath)
        dump(blurNormalize, self.blurRankPath)
        Log.d(f&#34;Visual rank length {len(motionNormalize)}  {len(blurNormalize)}&#34;)
        Log.i(f&#34;Visual ranking saved .............&#34;)

    def setVideoFps(self):
        &#34;&#34;&#34;
        Function to set the original video fps to cache
        &#34;&#34;&#34;
        self.cache.writeDataToCache(CACHE_FPS, self.fps)

    def setVideoFrameCount(self):
        &#34;&#34;&#34;
        Function to set the original video frame count to cache
        &#34;&#34;&#34;
        self.cache.writeDataToCache(CACHE_FRAME_COUNT, self.frameCount)

    def __del__(self):
        &#34;&#34;&#34;
        Clean ups
        &#34;&#34;&#34;
        del self.cache
        if self.videoGetter is not None:
            del self.videoGetter

        Log.d(&#34;Cleaning up.&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="lib.visual.Visual"><code class="flex name class">
<span>class <span class="ident">Visual</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class to perform Visual Processing on the input video file. Motion and Blur detections
are used to calculate the rank. The ranks are per frame, so later the ranks are
normalized to sec</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>motionRankPath</code></strong> :&ensp;<code>str</code></dt>
<dd>file to store the motion rank</dd>
<dt><strong><code>blurRankPath</code></strong> :&ensp;<code>str</code></dt>
<dd>file to store the blur rank</dd>
<dt><strong><code>blurThreshold</code></strong> :&ensp;<code>int</code></dt>
<dd>threshold to rank the blur feature</dd>
<dt><strong><code>motionThreshold</code></strong> :&ensp;<code>int</code></dt>
<dd>threshold to rank the motion feature</dd>
<dt><strong><code>fps</code></strong> :&ensp;<code>float</code></dt>
<dd>input video fps</dd>
<dt><strong><code>frameCount</code></strong> :&ensp;<code>int</code></dt>
<dd>number of frames</dd>
<dt><strong><code>motion</code></strong> :&ensp;<code>list</code></dt>
<dd>list of the ranks for the motion feature</dd>
<dt><strong><code>blur</code></strong> :&ensp;<code>list</code></dt>
<dd>list of the ranks for the blur feature</dd>
<dt><strong><code>cache</code></strong> :&ensp;<code>Cache</code></dt>
<dd>cache object to store the data</dd>
<dt><strong><code>videoGetter</code></strong> :&ensp;<code>VideoGet</code></dt>
<dd>video reader object to read the video and save it in thread</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Visual:
    &#34;&#34;&#34;
    Class to perform Visual Processing on the input video file. Motion and Blur detections
    are used to calculate the rank. The ranks are per frame, so later the ranks are
    normalized to sec

    Attributes
    ----------
    motionRankPath : str
        file to store the motion rank
    blurRankPath : str
        file to store the blur rank
    blurThreshold : int
        threshold to rank the blur feature
    motionThreshold : int
        threshold to rank the motion feature
    fps : float
        input video fps
    frameCount : int
        number of frames
    motion : list
        list of the ranks for the motion feature
    blur : list
        list of the ranks for the blur feature
    cache : Cache
        cache object to store the data
    videoGetter : VideoGet
        video reader object to read the video and save it in thread
    &#34;&#34;&#34;

    def __init__(self):
        self.motionRankPath = os.path.join(os.getcwd(), RANK_DIR, RANK_OUT_MOTION)
        self.blurRankPath = os.path.join(os.getcwd(), RANK_DIR, RANK_OUT_BLUR)
        self.blurThreshold = BLUR_THRESHOLD
        self.motionThreshold = MOTION_THRESHOLD
        self.fps = None
        self.frameCount = None
        self.motion = None
        self.blur = None
        self.cache = Cache()
        self.videoGetter = None

    def detectBlur(self, image):
        &#34;&#34;&#34;
        Laplacian take 2nd derivative of one channel of the image(gray scale)
        It highlights regions of an image containing rapid intensity changes, much like the Sobel and Scharr operators.
        And then calculates the variance (squared SD), then check if the variance satisfies the Threshold value/

        Parameters
        ---------
        image : array
            frame from the video file
        &#34;&#34;&#34;
        # (h, w) = image.shape
        # (cX, cY) = (int(w / 2.0), int(h / 2.0))
        #
        # fft = np.fft.fft2(image)
        # fftShift = np.fft.fftshift(fft)
        #
        # fftShift[cY - size:cY + size, cX - size:cX + size] = 0
        # fftShift = np.fft.ifftshift(fftShift)
        # recon = np.fft.ifft2(fftShift)
        #
        # magnitude = 20 * np.log(np.abs(recon))
        # mean = np.mean(magnitude)
        # return mean &lt;= thresh
        if cv2.Laplacian(image, cv2.CV_64F).var() &gt;= self.blurThreshold:
            return RANK_BLUR
        return 0

    def startProcessing(self, inputFile, display=False):
        &#34;&#34;&#34;
        Function to run the processing on the Video file. Motion and Blur features are
        detected and based on that ranking is set

        Parameters
        ----------
        inputFile : str
            input video file
        display : bool
            True to display the video while processing
        &#34;&#34;&#34;

        if os.path.isfile(inputFile) is False:
            Log.e(f&#34;File {inputFile} does not exists&#34;)
            return

        # maintaining the motion and blur frames list
        self.motion = list()
        self.blur = list()

        self.videoGetter = VideoGet(str(inputFile)).start()
        myClip = self.videoGetter.stream

        if self.videoGetter.Q.qsize() == 0:
            time.sleep(1)
            Log.d(f&#34;Waiting for the buffer to fill up.&#34;)

        fps = myClip.get(cv2.CAP_PROP_FPS)
        totalFrames = myClip.get(cv2.CAP_PROP_FRAME_COUNT)

        self.fps = fps
        self.frameCount = totalFrames
        self.setVideoFps()
        self.setVideoFrameCount()

        # printing some info
        Log.d(f&#34;Total count of video frames :: {totalFrames}&#34;)
        Log.i(f&#34;Video fps :: {fps}&#34;)
        Log.i(f&#34;Bit rate :: {cv2.CAP_PROP_BITRATE}&#34;)
        Log.i(f&#34;Video format :: {cv2.CAP_PROP_FORMAT}&#34;)
        Log.i(f&#34;Video four cc :: {cv2.CAP_PROP_FOURCC}&#34;)

        count = 0
        firstFrame = self.videoGetter.read()
        firstFrameProcessed = True

        while self.videoGetter.more():
            frame = self.videoGetter.read()

            if frame is None:
                break

            original = frame
            count += 1

            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            self.blur.append(self.detectBlur(frame))
            frame = cv2.GaussianBlur(frame, (21, 21), 0)

            if firstFrameProcessed:
                firstFrame = cv2.cvtColor(firstFrame, cv2.COLOR_BGR2GRAY)
                firstFrame = cv2.GaussianBlur(firstFrame, (21, 21), 0)
                firstFrameProcessed = False

            frameDelta = cv2.absdiff(firstFrame, frame)
            thresh = cv2.threshold(frameDelta, self.motionThreshold, 255, cv2.THRESH_BINARY)[1]

            threshSum = thresh.sum()
            if threshSum &gt; 0:
                self.motion.append(RANK_MOTION)
            else:
                self.motion.append(0)

            if display:
                cv2.imshow(&#34;Video Feed&#34;, original)
                key = cv2.waitKey(1) &amp; 0xFF

                # if the `q` key is pressed, break from the loop
                if key == ord(&#34;q&#34;):
                    break

            # assigning the processed frame as the first frame to cal diff later on
            firstFrame = frame

        # clearing memory
        myClip.release()
        self.videoGetter.stop()
        cv2.destroyAllWindows()

        # calling the normalization of ranking
        self.timedRankingNormalize()

    def timedRankingNormalize(self):
        &#34;&#34;&#34;
        Since ranking is added to frames, since frames are duration * fps
        and audio frame system is different since frame are duration * rate
        so we need to generalize the ranking system
        sol: ranking sec of the video and audio, for than taking mean of the
        frames to generate rank for video.

        Since ranking is 0 or 1, the mean will be different and we get more versatile
        results.

        We will read both the list and slice the video to get 1 sec of frames(1 * fps) and get
        mean/average as the rank for the 1 sec

        &#34;&#34;&#34;
        motionNormalize = []
        blurNormalize = []
        for i in range(0, int(self.frameCount), int(self.fps)):
            if len(self.motion) &gt;= (i + int(self.fps)):
                motionNormalize.append(np.mean(self.motion[i: i + int(self.fps)]))
                blurNormalize.append(np.mean(self.blur[i: i + int(self.fps)]))
            else:
                break

        # saving all processed stuffs
        dump(motionNormalize, self.motionRankPath)
        dump(blurNormalize, self.blurRankPath)
        Log.d(f&#34;Visual rank length {len(motionNormalize)}  {len(blurNormalize)}&#34;)
        Log.i(f&#34;Visual ranking saved .............&#34;)

    def setVideoFps(self):
        &#34;&#34;&#34;
        Function to set the original video fps to cache
        &#34;&#34;&#34;
        self.cache.writeDataToCache(CACHE_FPS, self.fps)

    def setVideoFrameCount(self):
        &#34;&#34;&#34;
        Function to set the original video frame count to cache
        &#34;&#34;&#34;
        self.cache.writeDataToCache(CACHE_FRAME_COUNT, self.frameCount)

    def __del__(self):
        &#34;&#34;&#34;
        Clean ups
        &#34;&#34;&#34;
        del self.cache
        if self.videoGetter is not None:
            del self.videoGetter

        Log.d(&#34;Cleaning up.&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="lib.visual.Visual.detectBlur"><code class="name flex">
<span>def <span class="ident">detectBlur</span></span>(<span>self, image)</span>
</code></dt>
<dd>
<div class="desc"><p>Laplacian take 2nd derivative of one channel of the image(gray scale)
It highlights regions of an image containing rapid intensity changes, much like the Sobel and Scharr operators.
And then calculates the variance (squared SD), then check if the variance satisfies the Threshold value/</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>array</code></dt>
<dd>frame from the video file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detectBlur(self, image):
    &#34;&#34;&#34;
    Laplacian take 2nd derivative of one channel of the image(gray scale)
    It highlights regions of an image containing rapid intensity changes, much like the Sobel and Scharr operators.
    And then calculates the variance (squared SD), then check if the variance satisfies the Threshold value/

    Parameters
    ---------
    image : array
        frame from the video file
    &#34;&#34;&#34;
    # (h, w) = image.shape
    # (cX, cY) = (int(w / 2.0), int(h / 2.0))
    #
    # fft = np.fft.fft2(image)
    # fftShift = np.fft.fftshift(fft)
    #
    # fftShift[cY - size:cY + size, cX - size:cX + size] = 0
    # fftShift = np.fft.ifftshift(fftShift)
    # recon = np.fft.ifft2(fftShift)
    #
    # magnitude = 20 * np.log(np.abs(recon))
    # mean = np.mean(magnitude)
    # return mean &lt;= thresh
    if cv2.Laplacian(image, cv2.CV_64F).var() &gt;= self.blurThreshold:
        return RANK_BLUR
    return 0</code></pre>
</details>
</dd>
<dt id="lib.visual.Visual.setVideoFps"><code class="name flex">
<span>def <span class="ident">setVideoFps</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to set the original video fps to cache</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setVideoFps(self):
    &#34;&#34;&#34;
    Function to set the original video fps to cache
    &#34;&#34;&#34;
    self.cache.writeDataToCache(CACHE_FPS, self.fps)</code></pre>
</details>
</dd>
<dt id="lib.visual.Visual.setVideoFrameCount"><code class="name flex">
<span>def <span class="ident">setVideoFrameCount</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to set the original video frame count to cache</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setVideoFrameCount(self):
    &#34;&#34;&#34;
    Function to set the original video frame count to cache
    &#34;&#34;&#34;
    self.cache.writeDataToCache(CACHE_FRAME_COUNT, self.frameCount)</code></pre>
</details>
</dd>
<dt id="lib.visual.Visual.startProcessing"><code class="name flex">
<span>def <span class="ident">startProcessing</span></span>(<span>self, inputFile, display=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to run the processing on the Video file. Motion and Blur features are
detected and based on that ranking is set</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>inputFile</code></strong> :&ensp;<code>str</code></dt>
<dd>input video file</dd>
<dt><strong><code>display</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to display the video while processing</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def startProcessing(self, inputFile, display=False):
    &#34;&#34;&#34;
    Function to run the processing on the Video file. Motion and Blur features are
    detected and based on that ranking is set

    Parameters
    ----------
    inputFile : str
        input video file
    display : bool
        True to display the video while processing
    &#34;&#34;&#34;

    if os.path.isfile(inputFile) is False:
        Log.e(f&#34;File {inputFile} does not exists&#34;)
        return

    # maintaining the motion and blur frames list
    self.motion = list()
    self.blur = list()

    self.videoGetter = VideoGet(str(inputFile)).start()
    myClip = self.videoGetter.stream

    if self.videoGetter.Q.qsize() == 0:
        time.sleep(1)
        Log.d(f&#34;Waiting for the buffer to fill up.&#34;)

    fps = myClip.get(cv2.CAP_PROP_FPS)
    totalFrames = myClip.get(cv2.CAP_PROP_FRAME_COUNT)

    self.fps = fps
    self.frameCount = totalFrames
    self.setVideoFps()
    self.setVideoFrameCount()

    # printing some info
    Log.d(f&#34;Total count of video frames :: {totalFrames}&#34;)
    Log.i(f&#34;Video fps :: {fps}&#34;)
    Log.i(f&#34;Bit rate :: {cv2.CAP_PROP_BITRATE}&#34;)
    Log.i(f&#34;Video format :: {cv2.CAP_PROP_FORMAT}&#34;)
    Log.i(f&#34;Video four cc :: {cv2.CAP_PROP_FOURCC}&#34;)

    count = 0
    firstFrame = self.videoGetter.read()
    firstFrameProcessed = True

    while self.videoGetter.more():
        frame = self.videoGetter.read()

        if frame is None:
            break

        original = frame
        count += 1

        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        self.blur.append(self.detectBlur(frame))
        frame = cv2.GaussianBlur(frame, (21, 21), 0)

        if firstFrameProcessed:
            firstFrame = cv2.cvtColor(firstFrame, cv2.COLOR_BGR2GRAY)
            firstFrame = cv2.GaussianBlur(firstFrame, (21, 21), 0)
            firstFrameProcessed = False

        frameDelta = cv2.absdiff(firstFrame, frame)
        thresh = cv2.threshold(frameDelta, self.motionThreshold, 255, cv2.THRESH_BINARY)[1]

        threshSum = thresh.sum()
        if threshSum &gt; 0:
            self.motion.append(RANK_MOTION)
        else:
            self.motion.append(0)

        if display:
            cv2.imshow(&#34;Video Feed&#34;, original)
            key = cv2.waitKey(1) &amp; 0xFF

            # if the `q` key is pressed, break from the loop
            if key == ord(&#34;q&#34;):
                break

        # assigning the processed frame as the first frame to cal diff later on
        firstFrame = frame

    # clearing memory
    myClip.release()
    self.videoGetter.stop()
    cv2.destroyAllWindows()

    # calling the normalization of ranking
    self.timedRankingNormalize()</code></pre>
</details>
</dd>
<dt id="lib.visual.Visual.timedRankingNormalize"><code class="name flex">
<span>def <span class="ident">timedRankingNormalize</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Since ranking is added to frames, since frames are duration * fps
and audio frame system is different since frame are duration * rate
so we need to generalize the ranking system
sol: ranking sec of the video and audio, for than taking mean of the
frames to generate rank for video.</p>
<p>Since ranking is 0 or 1, the mean will be different and we get more versatile
results.</p>
<p>We will read both the list and slice the video to get 1 sec of frames(1 * fps) and get
mean/average as the rank for the 1 sec</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def timedRankingNormalize(self):
    &#34;&#34;&#34;
    Since ranking is added to frames, since frames are duration * fps
    and audio frame system is different since frame are duration * rate
    so we need to generalize the ranking system
    sol: ranking sec of the video and audio, for than taking mean of the
    frames to generate rank for video.

    Since ranking is 0 or 1, the mean will be different and we get more versatile
    results.

    We will read both the list and slice the video to get 1 sec of frames(1 * fps) and get
    mean/average as the rank for the 1 sec

    &#34;&#34;&#34;
    motionNormalize = []
    blurNormalize = []
    for i in range(0, int(self.frameCount), int(self.fps)):
        if len(self.motion) &gt;= (i + int(self.fps)):
            motionNormalize.append(np.mean(self.motion[i: i + int(self.fps)]))
            blurNormalize.append(np.mean(self.blur[i: i + int(self.fps)]))
        else:
            break

    # saving all processed stuffs
    dump(motionNormalize, self.motionRankPath)
    dump(blurNormalize, self.blurRankPath)
    Log.d(f&#34;Visual rank length {len(motionNormalize)}  {len(blurNormalize)}&#34;)
    Log.i(f&#34;Visual ranking saved .............&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="lib" href="index.html">lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="lib.visual.Visual" href="#lib.visual.Visual">Visual</a></code></h4>
<ul class="">
<li><code><a title="lib.visual.Visual.detectBlur" href="#lib.visual.Visual.detectBlur">detectBlur</a></code></li>
<li><code><a title="lib.visual.Visual.setVideoFps" href="#lib.visual.Visual.setVideoFps">setVideoFps</a></code></li>
<li><code><a title="lib.visual.Visual.setVideoFrameCount" href="#lib.visual.Visual.setVideoFrameCount">setVideoFrameCount</a></code></li>
<li><code><a title="lib.visual.Visual.startProcessing" href="#lib.visual.Visual.startProcessing">startProcessing</a></code></li>
<li><code><a title="lib.visual.Visual.timedRankingNormalize" href="#lib.visual.Visual.timedRankingNormalize">timedRankingNormalize</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>